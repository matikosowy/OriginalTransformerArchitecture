# Original Transformer's Architecture

Implementation of the original Transformer model's architecture from the paper <a href='https://arxiv.org/abs/1706.03762'>Attention is All You Need</a>. 

I've used pure PyTorch to create the revolutionary Transformer model. I've also implemented a test program, where the model learns to predict next token in a sentence. This architecture is widely used in Seq2Seq tasks like translation. Moreover, its modifications, like using only the Encoder or only the Decoder part, thrive in LLMs like ChatGPT.


@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}
